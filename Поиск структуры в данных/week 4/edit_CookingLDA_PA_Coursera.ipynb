{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment: \n",
    "## Готовим LDA по рецептам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как вы уже знаете, в тематическом моделировании делается предположение о том, что для определения тематики порядок слов в документе не важен; об этом гласит гипотеза «мешка слов». Сегодня мы будем работать с несколько нестандартной для тематического моделирования коллекцией, которую можно назвать «мешком ингредиентов», потому что на состоит из рецептов блюд разных кухонь. Тематические модели ищут слова, которые часто вместе встречаются в документах, и составляют из них темы. Мы попробуем применить эту идею к рецептам и найти кулинарные «темы». Эта коллекция хороша тем, что не требует предобработки. Кроме того, эта задача достаточно наглядно иллюстрирует принцип работы тематических моделей.\n",
    "\n",
    "Для выполнения заданий, помимо часто используемых в курсе библиотек, потребуются модули *json* и *gensim*. Первый входит в дистрибутив Anaconda, второй можно поставить командой \n",
    "\n",
    "*pip install gensim*\n",
    "\n",
    "Построение модели занимает некоторое время. На ноутбуке с процессором Intel Core i7 и тактовой частотой 2400 МГц на построение одной модели уходит менее 10 минут."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Коллекция дана в json-формате: для каждого рецепта известны его id, кухня (cuisine) и список ингредиентов, в него входящих. Загрузить данные можно с помощью модуля json (он входит в дистрибутив Anaconda):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"recipes.json\") as f:\n",
    "    recipes = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 10259, 'cuisine': 'greek', 'ingredients': ['romaine lettuce', 'black olives', 'grape tomatoes', 'garlic', 'pepper', 'purple onion', 'seasoning', 'garbanzo beans', 'feta cheese crumbles']}\n"
     ]
    }
   ],
   "source": [
    "print(recipes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Составление корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/agekyan/.conda/envs/psheenv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/agekyan/.conda/envs/psheenv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/agekyan/.conda/envs/psheenv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/agekyan/.conda/envs/psheenv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/agekyan/.conda/envs/psheenv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/agekyan/.conda/envs/psheenv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/agekyan/.conda/envs/psheenv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/agekyan/.conda/envs/psheenv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/agekyan/.conda/envs/psheenv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/agekyan/.conda/envs/psheenv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/agekyan/.conda/envs/psheenv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/agekyan/.conda/envs/psheenv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Slow version of gensim.models.doc2vec is being used\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша коллекция небольшая, и целиком помещается в оперативную память. Gensim может работать с такими данными и не требует их сохранения на диск в специальном формате. Для этого коллекция должна быть представлена в виде списка списков, каждый внутренний список соответствует отдельному документу и состоит из его слов. Пример коллекции из двух документов: \n",
    "\n",
    "[[\"hello\", \"world\"], [\"programming\", \"in\", \"python\"]]\n",
    "\n",
    "Преобразуем наши данные в такой формат, а затем создадим объекты corpus и dictionary, с которыми будет работать модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [recipe[\"ingredients\"] for recipe in recipes]\n",
    "dictionary = corpora.Dictionary(texts)   # составляем словарь\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]  # составляем корпус документов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['romaine lettuce', 'black olives', 'grape tomatoes', 'garlic', 'pepper', 'purple onion', 'seasoning', 'garbanzo beans', 'feta cheese crumbles']\n",
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(texts[0])\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У объекта dictionary есть полезная переменная dictionary.token2id, позволяющая находить соответствие между ингредиентами и их индексами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение модели\n",
    "Вам может понадобиться [документация](https://radimrehurek.com/gensim/models/ldamodel.html) LDA в gensim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 1.__ Обучите модель LDA с 40 темами, установив количество проходов по коллекции 5 и оставив остальные параметры по умолчанию. \n",
    "\n",
    "\n",
    "Затем вызовите метод модели *show_topics*, указав количество тем 40 и количество токенов 10, и сохраните результат (топы ингредиентов в темах) в отдельную переменную. Если при вызове метода *show_topics* указать параметр *formatted=True*, то топы ингредиентов будет удобно выводить на печать, если *formatted=False*, будет удобно работать со списком программно. Выведите топы на печать, рассмотрите темы, а затем ответьте на вопрос:\n",
    "\n",
    "Сколько раз ингредиенты \"salt\", \"sugar\", \"water\", \"mushrooms\", \"chicken\", \"eggs\" встретились среди топов-10 всех 40 тем? При ответе __не нужно__ учитывать составные ингредиенты, например, \"hot water\".\n",
    "\n",
    "Передайте 6 чисел в функцию save_answers1 и загрузите сгенерированный файл в форму.\n",
    "\n",
    "У gensim нет возможности фиксировать случайное приближение через параметры метода, но библиотека использует numpy для инициализации матриц. Поэтому, по утверждению автора библиотеки, фиксировать случайное приближение нужно командой, которая написана в следующей ячейке. __Перед строкой кода с построением модели обязательно вставляйте указанную строку фиксации random.seed.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(76543)\n",
    "# здесь код для построения модели:\n",
    "lda = models.LdaModel(corpus, num_topics=40, passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [('212', 0.05225377977993424),\n",
       "   ('362', 0.03570444578638662),\n",
       "   ('26', 0.03528551365350228),\n",
       "   ('228', 0.03339553838559049),\n",
       "   ('105', 0.03308297449307879),\n",
       "   ('669', 0.032542235493637404),\n",
       "   ('161', 0.03177142423745316),\n",
       "   ('629', 0.03176279524663153),\n",
       "   ('19', 0.030535098207290702),\n",
       "   ('89', 0.028073506396233978)]),\n",
       " (1,\n",
       "  [('204', 0.0938830046398903),\n",
       "   ('47', 0.07723599236949485),\n",
       "   ('11', 0.06928966859655218),\n",
       "   ('115', 0.0645597120128743),\n",
       "   ('50', 0.050080548513999554),\n",
       "   ('312', 0.04346102340638939),\n",
       "   ('27', 0.03620340334014721),\n",
       "   ('365', 0.02653907969609199),\n",
       "   ('531', 0.026498368386956846),\n",
       "   ('52', 0.023663597571690596)]),\n",
       " (2,\n",
       "  [('146', 0.08554318370976846),\n",
       "   ('475', 0.07927953110153554),\n",
       "   ('555', 0.07324429656800735),\n",
       "   ('48', 0.06416299880996343),\n",
       "   ('309', 0.05619921961913635),\n",
       "   ('124', 0.042188619394084616),\n",
       "   ('781', 0.03012220302057079),\n",
       "   ('652', 0.03005543275062138),\n",
       "   ('200', 0.02472166266811287),\n",
       "   ('613', 0.022668417466848244)]),\n",
       " (3,\n",
       "  [('11', 0.07862716665394198),\n",
       "   ('40', 0.06727561035542515),\n",
       "   ('35', 0.052589545125533675),\n",
       "   ('22', 0.05025020908917201),\n",
       "   ('43', 0.03762584789399659),\n",
       "   ('38', 0.036137196412243416),\n",
       "   ('428', 0.033591278917313235),\n",
       "   ('12', 0.033077434529842964),\n",
       "   ('351', 0.03122907644351581),\n",
       "   ('560', 0.028538038884999863)]),\n",
       " (4,\n",
       "  [('213', 0.07461745192304758),\n",
       "   ('35', 0.06479690972993947),\n",
       "   ('250', 0.06035078325429822),\n",
       "   ('200', 0.056973804793604),\n",
       "   ('401', 0.042834531717067685),\n",
       "   ('11', 0.04071399994001524),\n",
       "   ('3', 0.03902662849158427),\n",
       "   ('181', 0.03813912364259293),\n",
       "   ('13', 0.030976818884719745),\n",
       "   ('29', 0.026643589386320752)]),\n",
       " (5,\n",
       "  [('78', 0.11982672143816918),\n",
       "   ('103', 0.08288734786178828),\n",
       "   ('58', 0.05269690100877692),\n",
       "   ('105', 0.052118575695723854),\n",
       "   ('13', 0.03863745829949932),\n",
       "   ('171', 0.037406519720465735),\n",
       "   ('32', 0.03568376653666744),\n",
       "   ('11', 0.03395178045781705),\n",
       "   ('376', 0.03144353223507235),\n",
       "   ('79', 0.027508208874170326)]),\n",
       " (6,\n",
       "  [('699', 0.08388384856463739),\n",
       "   ('953', 0.03926943286546218),\n",
       "   ('1413', 0.03758443622470028),\n",
       "   ('1001', 0.02943986322904384),\n",
       "   ('484', 0.026924602830891393),\n",
       "   ('573', 0.026325077781043354),\n",
       "   ('770', 0.022061640420745265),\n",
       "   ('942', 0.021308866955497007),\n",
       "   ('200', 0.021259487667920275),\n",
       "   ('1178', 0.020778344611640368)]),\n",
       " (7,\n",
       "  [('255', 0.10628438053037195),\n",
       "   ('996', 0.07289011286613993),\n",
       "   ('185', 0.06817380946191709),\n",
       "   ('69', 0.06305420871056365),\n",
       "   ('668', 0.03872132293569087),\n",
       "   ('1856', 0.03756353680657123),\n",
       "   ('483', 0.03668471546842948),\n",
       "   ('1085', 0.03589097216671408),\n",
       "   ('982', 0.03424882563730921),\n",
       "   ('1397', 0.031388720242122053)]),\n",
       " (8,\n",
       "  [('60', 0.08745959962673377),\n",
       "   ('466', 0.07086751577272168),\n",
       "   ('205', 0.049044334599929996),\n",
       "   ('29', 0.04761807942234747),\n",
       "   ('1082', 0.040183357999623345),\n",
       "   ('330', 0.03705867607245235),\n",
       "   ('1089', 0.03068027405437452),\n",
       "   ('886', 0.028344237621843297),\n",
       "   ('1265', 0.02397139354869799),\n",
       "   ('47', 0.023660368998698837)]),\n",
       " (9,\n",
       "  [('494', 0.09148006481723105),\n",
       "   ('324', 0.0633762761437254),\n",
       "   ('311', 0.045006517053087766),\n",
       "   ('459', 0.0436404836944381),\n",
       "   ('301', 0.042442933986255295),\n",
       "   ('102', 0.03979496606235263),\n",
       "   ('47', 0.03608462204872711),\n",
       "   ('248', 0.036015160762833086),\n",
       "   ('119', 0.034720746463960506),\n",
       "   ('2029', 0.03460085757040422)]),\n",
       " (10,\n",
       "  [('252', 0.07823952897389927),\n",
       "   ('327', 0.04897501958778712),\n",
       "   ('105', 0.04648378463702632),\n",
       "   ('208', 0.03577419144524405),\n",
       "   ('54', 0.03378620421260391),\n",
       "   ('78', 0.03285736470057002),\n",
       "   ('10', 0.03076736429031434),\n",
       "   ('35', 0.02999701588816526),\n",
       "   ('335', 0.02721264135124731),\n",
       "   ('11', 0.024872809348927116)]),\n",
       " (11,\n",
       "  [('54', 0.09106853373036261),\n",
       "   ('190', 0.07431670834582274),\n",
       "   ('308', 0.06079241199569028),\n",
       "   ('27', 0.04999116004529327),\n",
       "   ('11', 0.045974806227243876),\n",
       "   ('13', 0.038794204850076695),\n",
       "   ('209', 0.036524441752087355),\n",
       "   ('3', 0.03517710233585811),\n",
       "   ('35', 0.02401639608037581),\n",
       "   ('315', 0.023502223064482464)]),\n",
       " (12,\n",
       "  [('393', 0.0968863284709254),\n",
       "   ('522', 0.08010898047503914),\n",
       "   ('988', 0.054091361184645595),\n",
       "   ('938', 0.05106821478488482),\n",
       "   ('485', 0.04531833089706968),\n",
       "   ('803', 0.044856212704221764),\n",
       "   ('566', 0.0422826243266292),\n",
       "   ('333', 0.04113417059093057),\n",
       "   ('774', 0.02784626667044924),\n",
       "   ('35', 0.027223960265801064)]),\n",
       " (13,\n",
       "  [('35', 0.0755938162961754),\n",
       "   ('194', 0.06478002195576267),\n",
       "   ('139', 0.048772471182578256),\n",
       "   ('11', 0.036272246716715875),\n",
       "   ('54', 0.0290770967165223),\n",
       "   ('551', 0.028991328562550798),\n",
       "   ('105', 0.02629565959986205),\n",
       "   ('29', 0.025936210976554453),\n",
       "   ('3', 0.025668374506371695),\n",
       "   ('140', 0.02554838238385221)]),\n",
       " (14,\n",
       "  [('108', 0.11935206021054584),\n",
       "   ('465', 0.0681090970159873),\n",
       "   ('162', 0.05345127520063173),\n",
       "   ('796', 0.05263897758983473),\n",
       "   ('876', 0.052323027140613855),\n",
       "   ('3', 0.03958105053296464),\n",
       "   ('473', 0.037166848349854884),\n",
       "   ('54', 0.031156205462067287),\n",
       "   ('1343', 0.029935356303075675),\n",
       "   ('269', 0.02751896033515101)]),\n",
       " (15,\n",
       "  [('191', 0.1848917220061298),\n",
       "   ('312', 0.09308305447400939),\n",
       "   ('348', 0.05974950710046157),\n",
       "   ('991', 0.050518508432782785),\n",
       "   ('79', 0.038815813730283595),\n",
       "   ('27', 0.037246505848654424),\n",
       "   ('949', 0.03281597242583136),\n",
       "   ('856', 0.03055835116355604),\n",
       "   ('724', 0.03042396289320792),\n",
       "   ('460', 0.026067268098840597)]),\n",
       " (16,\n",
       "  [('57', 0.06546552690367215),\n",
       "   ('11', 0.04766156935803679),\n",
       "   ('75', 0.0467157233639792),\n",
       "   ('41', 0.044667168624650186),\n",
       "   ('111', 0.043666481182868075),\n",
       "   ('519', 0.036203808338745626),\n",
       "   ('54', 0.029682203372687652),\n",
       "   ('38', 0.029673571621789952),\n",
       "   ('3', 0.028800372375394893),\n",
       "   ('71', 0.02804540237967027)]),\n",
       " (17,\n",
       "  [('357', 0.06665175358518124),\n",
       "   ('74', 0.056289507075088674),\n",
       "   ('221', 0.045755318965266356),\n",
       "   ('112', 0.04512669366925564),\n",
       "   ('56', 0.04215665758978309),\n",
       "   ('3', 0.03812475327367122),\n",
       "   ('839', 0.036038251270948826),\n",
       "   ('242', 0.03142362989498387),\n",
       "   ('731', 0.028620744495332008),\n",
       "   ('337', 0.027976867897148143)]),\n",
       " (18,\n",
       "  [('184', 0.15246458960008016),\n",
       "   ('215', 0.04695696840969942),\n",
       "   ('316', 0.04590111533862254),\n",
       "   ('27', 0.037304909060918795),\n",
       "   ('11', 0.03605315858113421),\n",
       "   ('177', 0.0331415541867936),\n",
       "   ('875', 0.030587648091226362),\n",
       "   ('15', 0.02945845047061797),\n",
       "   ('236', 0.027372105549226346),\n",
       "   ('278', 0.02485940492629189)]),\n",
       " (19,\n",
       "  [('649', 0.07563638681970347),\n",
       "   ('431', 0.06881720107681726),\n",
       "   ('434', 0.056995011634977125),\n",
       "   ('253', 0.047732839728884875),\n",
       "   ('29', 0.04623299332764657),\n",
       "   ('129', 0.045745515136651795),\n",
       "   ('345', 0.037784914842535294),\n",
       "   ('621', 0.036088315341488444),\n",
       "   ('46', 0.028880854039954076),\n",
       "   ('243', 0.025937449961599916)]),\n",
       " (20,\n",
       "  [('277', 0.10102584465698837),\n",
       "   ('42', 0.08371435691003502),\n",
       "   ('247', 0.07601622205614764),\n",
       "   ('514', 0.0550607888674418),\n",
       "   ('72', 0.05192710239243538),\n",
       "   ('2028', 0.04495175617532788),\n",
       "   ('415', 0.04296329805855298),\n",
       "   ('869', 0.039964689235921926),\n",
       "   ('97', 0.03854349559274146),\n",
       "   ('496', 0.029073058765607916)]),\n",
       " (21,\n",
       "  [('115', 0.11174197149937563),\n",
       "   ('11', 0.10956752697217097),\n",
       "   ('15', 0.10723544195519753),\n",
       "   ('18', 0.09842584920145636),\n",
       "   ('27', 0.07253897562062998),\n",
       "   ('53', 0.05906606674363853),\n",
       "   ('65', 0.05099110910781087),\n",
       "   ('47', 0.041013074429469984),\n",
       "   ('290', 0.03796251241700553),\n",
       "   ('310', 0.03373265338557483)]),\n",
       " (22,\n",
       "  [('187', 0.06575582477558765),\n",
       "   ('706', 0.06319608634963411),\n",
       "   ('651', 0.041477519905578734),\n",
       "   ('1031', 0.04066342443476917),\n",
       "   ('464', 0.0393977938748447),\n",
       "   ('521', 0.03366287473242917),\n",
       "   ('83', 0.031267993764811215),\n",
       "   ('380', 0.03060890078349156),\n",
       "   ('608', 0.029998612387847554),\n",
       "   ('164', 0.025705213939982586)]),\n",
       " (23,\n",
       "  [('229', 0.15399060048071828),\n",
       "   ('899', 0.053156200434025476),\n",
       "   ('1172', 0.05176589087926995),\n",
       "   ('794', 0.03844786386494519),\n",
       "   ('1454', 0.03409364514446851),\n",
       "   ('321', 0.02756283940916965),\n",
       "   ('157', 0.026083174411677432),\n",
       "   ('33', 0.0254298723573705),\n",
       "   ('470', 0.022550755074876163),\n",
       "   ('1605', 0.021231291369423776)]),\n",
       " (24,\n",
       "  [('41', 0.05772072684238177),\n",
       "   ('11', 0.050851474210085824),\n",
       "   ('35', 0.04839080665205405),\n",
       "   ('109', 0.04443388396470238),\n",
       "   ('347', 0.03479897917958925),\n",
       "   ('3', 0.0334111156245477),\n",
       "   ('19', 0.032284872480950876),\n",
       "   ('123', 0.029134480178034945),\n",
       "   ('29', 0.02872334240663049),\n",
       "   ('90', 0.027441712936088956)]),\n",
       " (25,\n",
       "  [('597', 0.060945962208466015),\n",
       "   ('439', 0.054586699152843234),\n",
       "   ('47', 0.054112013434788345),\n",
       "   ('132', 0.04523629361131309),\n",
       "   ('312', 0.042739340712811276),\n",
       "   ('320', 0.0376940810777876),\n",
       "   ('11', 0.03691083290657501),\n",
       "   ('579', 0.036276781550506385),\n",
       "   ('50', 0.03557563112222394),\n",
       "   ('115', 0.035241735907189035)]),\n",
       " (26,\n",
       "  [('49', 0.13156514599320543),\n",
       "   ('372', 0.06692008527455177),\n",
       "   ('387', 0.06592434185950669),\n",
       "   ('1044', 0.054965974932950024),\n",
       "   ('541', 0.05230868795677273),\n",
       "   ('1108', 0.045237068909074564),\n",
       "   ('821', 0.04213859815801577),\n",
       "   ('389', 0.037778543302875355),\n",
       "   ('1212', 0.027475671594684906),\n",
       "   ('1077', 0.024362147609659896)]),\n",
       " (27,\n",
       "  [('437', 0.07491608844066204),\n",
       "   ('188', 0.059562181535468356),\n",
       "   ('804', 0.03625116782375787),\n",
       "   ('273', 0.035269810520569385),\n",
       "   ('1099', 0.03518053199117325),\n",
       "   ('1349', 0.03180458637711517),\n",
       "   ('742', 0.02993923815405846),\n",
       "   ('1173', 0.0269224225503429),\n",
       "   ('295', 0.026798780249474845),\n",
       "   ('298', 0.02304028589488744)]),\n",
       " (28,\n",
       "  [('221', 0.0652238793762112),\n",
       "   ('145', 0.06193021419089717),\n",
       "   ('47', 0.0434965074825737),\n",
       "   ('32', 0.029729175436358512),\n",
       "   ('864', 0.027444557284810663),\n",
       "   ('29', 0.022742519242717772),\n",
       "   ('560', 0.022072688029939943),\n",
       "   ('19', 0.02178017589799346),\n",
       "   ('222', 0.02076795268747481),\n",
       "   ('105', 0.02059403233452781)]),\n",
       " (29,\n",
       "  [('11', 0.08227297248821744),\n",
       "   ('122', 0.08026216384116681),\n",
       "   ('105', 0.07905118912705314),\n",
       "   ('280', 0.07078043723675623),\n",
       "   ('54', 0.05465696589960357),\n",
       "   ('31', 0.03488472991325698),\n",
       "   ('13', 0.034512677015565055),\n",
       "   ('214', 0.03285605618311592),\n",
       "   ('29', 0.03212568916261062),\n",
       "   ('116', 0.032009427695359166)]),\n",
       " (30,\n",
       "  [('272', 0.10342387669423314),\n",
       "   ('245', 0.07029444059859767),\n",
       "   ('201', 0.061122404750098336),\n",
       "   ('577', 0.05295652840582601),\n",
       "   ('323', 0.04207130893762923),\n",
       "   ('228', 0.037285823880517634),\n",
       "   ('244', 0.03702535605091972),\n",
       "   ('696', 0.029653710706489766),\n",
       "   ('193', 0.026585348063925096),\n",
       "   ('728', 0.024602739291003126)]),\n",
       " (31,\n",
       "  [('24', 0.19008382882996494),\n",
       "   ('34', 0.10066293144905815),\n",
       "   ('216', 0.07131413314264613),\n",
       "   ('772', 0.03762714399222959),\n",
       "   ('13', 0.03493948161283723),\n",
       "   ('31', 0.033745831417178705),\n",
       "   ('38', 0.029231811686196516),\n",
       "   ('82', 0.026523970516414353),\n",
       "   ('11', 0.02539078192936169),\n",
       "   ('79', 0.025078627762907858)]),\n",
       " (32,\n",
       "  [('397', 0.09534227211031561),\n",
       "   ('863', 0.06311689115235092),\n",
       "   ('47', 0.06283102051724407),\n",
       "   ('524', 0.05343113338804649),\n",
       "   ('29', 0.03671646436506783),\n",
       "   ('11', 0.035312849379686095),\n",
       "   ('1218', 0.0323220364081793),\n",
       "   ('444', 0.025688464887168885),\n",
       "   ('9', 0.024538889710597835),\n",
       "   ('1339', 0.021080144213567114)]),\n",
       " (33,\n",
       "  [('4', 0.21564663351551105),\n",
       "   ('11', 0.14859668724117805),\n",
       "   ('35', 0.08582810539258447),\n",
       "   ('3', 0.05554959740713436),\n",
       "   ('29', 0.04195018913640605),\n",
       "   ('207', 0.040122982473745565),\n",
       "   ('54', 0.02925117732698522),\n",
       "   ('730', 0.02165074233896874),\n",
       "   ('12', 0.018928782714546222),\n",
       "   ('154', 0.018537006987093655)]),\n",
       " (34,\n",
       "  [('313', 0.14888497321817382),\n",
       "   ('37', 0.09814601958465038),\n",
       "   ('235', 0.05595311961364835),\n",
       "   ('20', 0.05259534473031302),\n",
       "   ('916', 0.04148722470664579),\n",
       "   ('206', 0.033037058310197764),\n",
       "   ('835', 0.030164910106981475),\n",
       "   ('13', 0.024305771347451915),\n",
       "   ('1291', 0.018759130380186124),\n",
       "   ('54', 0.017751697389989635)]),\n",
       " (35,\n",
       "  [('26', 0.09619102016000548),\n",
       "   ('94', 0.052451857492505444),\n",
       "   ('3', 0.04701619653299422),\n",
       "   ('99', 0.04653374528396252),\n",
       "   ('47', 0.040560845692566794),\n",
       "   ('228', 0.033056355416618284),\n",
       "   ('95', 0.03230689012593324),\n",
       "   ('29', 0.03202853098637103),\n",
       "   ('351', 0.030494165038522304),\n",
       "   ('11', 0.027553681383201148)]),\n",
       " (36,\n",
       "  [('233', 0.085570716102895),\n",
       "   ('1049', 0.08286759257556084),\n",
       "   ('1276', 0.06645266309474514),\n",
       "   ('1874', 0.055002070183464306),\n",
       "   ('737', 0.05209491410867095),\n",
       "   ('530', 0.03468398154317352),\n",
       "   ('1030', 0.031354179646145154),\n",
       "   ('1559', 0.020776493873189927),\n",
       "   ('976', 0.018875295513678413),\n",
       "   ('68', 0.018727766930004445)]),\n",
       " (37,\n",
       "  [('5', 0.07889990284166677),\n",
       "   ('54', 0.05587122760410924),\n",
       "   ('78', 0.04952895519527121),\n",
       "   ('11', 0.04724314275346665),\n",
       "   ('489', 0.04625787765583914),\n",
       "   ('110', 0.04440951725279304),\n",
       "   ('105', 0.040067065495693825),\n",
       "   ('12', 0.03699953863060124),\n",
       "   ('77', 0.027065835791397156),\n",
       "   ('535', 0.02651238816467918)]),\n",
       " (38,\n",
       "  [('54', 0.05850111458067736),\n",
       "   ('11', 0.044451975627327875),\n",
       "   ('82', 0.04183689087549259),\n",
       "   ('190', 0.040818397215222034),\n",
       "   ('3', 0.03568309019125312),\n",
       "   ('230', 0.0330448423996627),\n",
       "   ('391', 0.029810245951665523),\n",
       "   ('261', 0.02769765016999977),\n",
       "   ('477', 0.02500170771411605),\n",
       "   ('77', 0.022182087470271204)]),\n",
       " (39,\n",
       "  [('658', 0.10770694867988477),\n",
       "   ('1125', 0.06195723303416056),\n",
       "   ('656', 0.05622663685451216),\n",
       "   ('480', 0.05109423265079544),\n",
       "   ('664', 0.04293870916101534),\n",
       "   ('1396', 0.04130318316517988),\n",
       "   ('893', 0.035330595892066816),\n",
       "   ('7', 0.03268674417954435),\n",
       "   ('1137', 0.03053037637030578),\n",
       "   ('726', 0.02967612108367902)])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.show_topics(num_topics=40, num_words=10, formatted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_salt = 0\n",
    "c_sugar = 0\n",
    "c_water = 0\n",
    "c_mushrooms = 0\n",
    "c_chicken = 0\n",
    "c_eggs = 0\n",
    "\n",
    "for topic in lda.show_topics(num_topics=40, num_words=10, formatted=False):\n",
    "    for c in topic[1]:\n",
    "        if dictionary[int(c[0])] == 'salt':\n",
    "            c_salt += 1\n",
    "        elif dictionary[int(c[0])] == 'sugar':\n",
    "            c_sugar += 1\n",
    "        elif dictionary[int(c[0])] == 'water':\n",
    "            c_water += 1\n",
    "        elif dictionary[int(c[0])] == 'mushrooms':\n",
    "            c_mushrooms += 1\n",
    "        elif dictionary[int(c[0])] == 'chicken':\n",
    "            c_chicken += 1\n",
    "        elif dictionary[int(c[0])] == 'eggs':\n",
    "            c_eggs += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_answers1(c_salt, c_sugar, c_water, c_mushrooms, c_chicken, c_eggs):\n",
    "    with open(\"cooking_LDA_pa_task1.txt\", \"w\") as fout:\n",
    "        fout.write(\" \".join([str(el) for el in [c_salt, c_sugar, c_water, c_mushrooms, c_chicken, c_eggs]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 8, 10, 0, 1, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_answers1(c_salt, c_sugar, c_water, c_mushrooms, c_chicken, c_eggs)\n",
    "c_salt, c_sugar, c_water, c_mushrooms, c_chicken, c_eggs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Фильтрация словаря\n",
    "В топах тем гораздо чаще встречаются первые три рассмотренных ингредиента, чем последние три. При этом наличие в рецепте курицы, яиц и грибов яснее дает понять, что мы будем готовить, чем наличие соли, сахара и воды. Таким образом, даже в рецептах есть слова, часто встречающиеся в текстах и не несущие смысловой нагрузки, и поэтому их не желательно видеть в темах. Наиболее простой прием борьбы с такими фоновыми элементами — фильтрация словаря по частоте. Обычно словарь фильтруют с двух сторон: убирают очень редкие слова (в целях экономии памяти) и очень частые слова (в целях повышения интерпретируемости тем). Мы уберем только частые слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "dictionary2 = copy.deepcopy(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 2.__ У объекта dictionary2 есть переменная *dfs* — это словарь, ключами которого являются id токена, а элементами — число раз, сколько слово встретилось во всей коллекции. Сохраните в отдельный список ингредиенты, которые встретились в коллекции больше 4000 раз. Вызовите метод словаря *filter_tokens*, подав в качестве первого аргумента полученный список популярных ингредиентов. Вычислите две величины: dict_size_before и dict_size_after — размер словаря до и после фильтрации.\n",
    "\n",
    "Затем, используя новый словарь, создайте новый корпус документов, corpus2, по аналогии с тем, как это сделано в начале ноутбука. Вычислите две величины: corpus_size_before и corpus_size_after — суммарное количество ингредиентов в корпусе (для каждого документа вычислите число различных ингредиентов в нем и просуммируйте по всем документам) до и после фильтрации.\n",
    "\n",
    "Передайте величины dict_size_before, dict_size_after, corpus_size_before, corpus_size_after в функцию save_answers2 и загрузите сгенерированный файл в форму."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtr = [k for k in dictionary2.dfs.keys() if dictionary2.dfs[k] >= 4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 11, 13, 19, 27, 29, 35, 47, 54, 105, 115]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6714"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_size_before = len(dictionary2)\n",
    "dict_size_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary2.filter_tokens(filtr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6702"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_size_after = len(dictionary2)\n",
    "dict_size_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus2 = [dictionary2.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(428249, 343665)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_size_before = 0\n",
    "for corp in corpus:\n",
    "    corpus_size_before += len(corp)\n",
    "    \n",
    "corpus_size_after = 0\n",
    "for corp in corpus2:\n",
    "    corpus_size_after += len(corp)\n",
    "\n",
    "corpus_size_before, corpus_size_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_answers2(dict_size_before, dict_size_after, corpus_size_before, corpus_size_after):\n",
    "    with open(\"cooking_LDA_pa_task2.txt\", \"w\") as fout:\n",
    "        fout.write(\" \".join([str(el) for el in [dict_size_before, dict_size_after, corpus_size_before, corpus_size_after]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6714, 6702, 428249, 343665)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_answers2(dict_size_before, dict_size_after, corpus_size_before, corpus_size_after)\n",
    "dict_size_before, dict_size_after, corpus_size_before, corpus_size_after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение когерентностей\n",
    "__Задание 3.__ Постройте еще одну модель по корпусу corpus2 и словарю dictionary2, остальные параметры оставьте такими же, как при первом построении модели. Сохраните новую модель в другую переменную (не перезаписывайте предыдущую модель). Не забудьте про фиксирование seed!\n",
    "\n",
    "Затем воспользуйтесь методом *top_topics* модели, чтобы вычислить ее когерентность. Передайте в качестве аргумента соответствующий модели корпус. Метод вернет список кортежей (топ токенов, когерентность), отсортированных по убыванию последней. Вычислите среднюю по всем темам когерентность для каждой из двух моделей и передайте в функцию save_answers3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(76543)\n",
    "# здесь код для построения модели:\n",
    "lda2 = models.LdaModel(corpus2, num_topics=40, passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = lda.top_topics(corpus = corpus)\n",
    "tt2 = lda2.top_topics(corpus = corpus2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence = np.mean([t[1] for t in tt])\n",
    "coherence2 = np.mean([t[1] for t in tt2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_answers3(coherence, coherence2):\n",
    "    with open(\"cooking_LDA_pa_task3.txt\", \"w\") as fout:\n",
    "        fout.write(\" \".join([\"%3f\"%el for el in [coherence, coherence2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-703.2019367755069, -747.6959617371538)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_answers3(coherence, coherence2)\n",
    "coherence, coherence2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считается, что когерентность хорошо соотносится с человеческими оценками интерпретируемости тем. Поэтому на больших текстовых коллекциях когерентность обычно повышается, если убрать фоновую лексику. Однако в нашем случае этого не произошло. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Изучение влияния гиперпараметра alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом разделе мы будем работать со второй моделью, то есть той, которая построена по сокращенному корпусу. \n",
    "\n",
    "Пока что мы посмотрели только на матрицу темы-слова, теперь давайте посмотрим на матрицу темы-документы. Выведите темы для нулевого (или любого другого) документа из корпуса, воспользовавшись методом *get_document_topics* второй модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11, 0.10249999999999994),\n",
       " (17, 0.10249999999999994),\n",
       " (20, 0.10249999999999994),\n",
       " (22, 0.10249999999999994),\n",
       " (31, 0.5024999999999935)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda2.get_document_topics(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также выведите содержимое переменной *.alpha* второй модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025,\n",
       "       0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025,\n",
       "       0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025,\n",
       "       0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025,\n",
       "       0.025, 0.025, 0.025, 0.025])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda2.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У вас должно получиться, что документ характеризуется небольшим числом тем. Попробуем поменять гиперпараметр alpha, задающий априорное распределение Дирихле для распределений тем в документах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 4.__ Обучите третью модель: используйте сокращенный корпус (corpus2 и dictionary2) и установите параметр __alpha=1__, passes=5. Не забудьте про фиксацию seed! Выведите темы новой модели для нулевого документа; должно получиться, что распределение над множеством тем практически равномерное. Чтобы убедиться в том, что во второй модели документы описываются гораздо более разреженными распределениями, чем в третьей, посчитайте суммарное количество элементов, __превосходящих 0.01__, в матрицах темы-документы обеих моделей. Другими словами, запросите темы  модели для каждого документа с параметром *minimum_probability=0.01* и просуммируйте число элементов в получаемых массивах. Передайте две суммы (сначала для модели с alpha по умолчанию, затем для модели в alpha=1) в функцию save_answers4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(76543)\n",
    "# здесь код для построения модели:\n",
    "lda3 = models.LdaModel(corpus2, num_topics=40, passes=5, alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.020418393314420104),\n",
       " (1, 0.02043496061488068),\n",
       " (2, 0.02042243887958849),\n",
       " (3, 0.020487675806559018),\n",
       " (4, 0.020424065844942375),\n",
       " (5, 0.0204189120495762),\n",
       " (6, 0.02040816326530612),\n",
       " (7, 0.02040816848836313),\n",
       " (8, 0.024614245905064357),\n",
       " (9, 0.02040816326530612),\n",
       " (10, 0.020486659044261897),\n",
       " (11, 0.02043187187983818),\n",
       " (12, 0.02040816326530612),\n",
       " (13, 0.02091614247969255),\n",
       " (14, 0.02046236867183178),\n",
       " (15, 0.020415731982108318),\n",
       " (16, 0.020447556114291554),\n",
       " (17, 0.020420612623826145),\n",
       " (18, 0.0204103599136829),\n",
       " (19, 0.020421996687675702),\n",
       " (20, 0.04084319346614518),\n",
       " (21, 0.020421116429906037),\n",
       " (22, 0.02040816346949478),\n",
       " (23, 0.04055164531365445),\n",
       " (24, 0.02061895349967959),\n",
       " (25, 0.020467252738016407),\n",
       " (26, 0.020474216414603796),\n",
       " (27, 0.020410137874940233),\n",
       " (28, 0.020465499511951472),\n",
       " (29, 0.020571164096205036),\n",
       " (30, 0.082787746701213),\n",
       " (31, 0.07438229458472209),\n",
       " (32, 0.020658940786559266),\n",
       " (33, 0.020432346419415885),\n",
       " (34, 0.020408168373002692),\n",
       " (35, 0.020409717221070333),\n",
       " (36, 0.02040816326530612),\n",
       " (37, 0.020408470332851283),\n",
       " (38, 0.04111595296694455),\n",
       " (39, 0.020490206437796084)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda3.get_document_topics(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_model2 = 0\n",
    "for i in lda2.get_document_topics(corpus2, minimum_probability=0.01):\n",
    "    count_model2 += len(i)\n",
    "\n",
    "count_model3 = 0\n",
    "for i in lda3.get_document_topics(corpus2, minimum_probability=0.01):\n",
    "    count_model3 += len(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_answers4(count_model2, count_model3):\n",
    "    with open(\"cooking_LDA_pa_task4.txt\", \"w\") as fout:\n",
    "        fout.write(\" \".join([str(el) for el in [count_model2, count_model3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200742, 1590960)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_answers4(count_model2, count_model3)\n",
    "count_model2, count_model3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, гиперпараметр __alpha__ влияет на разреженность распределений тем в документах. Аналогично гиперпараметр __eta__ влияет на разреженность распределений слов в темах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA как способ понижения размерности\n",
    "Иногда, распределения над темами, найденные с помощью LDA, добавляют в матрицу объекты-признаки как дополнительные, семантические, признаки, и это может улучшить качество решения задачи. Для простоты давайте просто обучим классификатор рецептов на кухни на признаках, полученных из LDA, и измерим точность (accuracy).\n",
    "\n",
    "__Задание 5.__ Используйте модель, построенную по сокращенной выборке с alpha по умолчанию (вторую модель). Составьте матрицу $\\Theta = p(t|d)$ вероятностей тем в документах; вы можете использовать тот же метод get_document_topics, а также вектор правильных ответов y (в том же порядке, в котором рецепты идут в переменной recipes). Создайте объект RandomForestClassifier со 100 деревьями, с помощью функции cross_val_score вычислите среднюю accuracy по трем фолдам (перемешивать данные не нужно) и передайте в функцию save_answers5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "teta = np.zeros((len(corpus2), 40))\n",
    "for index, bow in enumerate(corpus2):\n",
    "    for topic, proba in lda2.get_document_topics(bow):\n",
    "        teta[index, topic] = proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [recipe['cuisine'] for recipe in recipes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agekyan/.conda/envs/psheenv/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=100)\n",
    "accuracy = np.mean(cross_val_score(model, teta, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_answers5(accuracy):\n",
    "     with open(\"cooking_LDA_pa_task5.txt\", \"w\") as fout:\n",
    "        fout.write(str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5689661169965947"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_answers5(accuracy)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для такого большого количества классов это неплохая точность. Вы можете попроовать обучать RandomForest на исходной матрице частот слов, имеющей значительно большую размерность, и увидеть, что accuracy увеличивается на 10–15%. Таким образом, LDA собрал не всю, но достаточно большую часть информации из выборки, в матрице низкого ранга."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA — вероятностная модель\n",
    "Матричное разложение, использующееся в LDA, интерпретируется как следующий процесс генерации документов.\n",
    "\n",
    "Для документа $d$ длины $n_d$:\n",
    "1. Из априорного распределения Дирихле с параметром alpha сгенерировать распределение над множеством тем: $\\theta_d \\sim Dirichlet(\\alpha)$\n",
    "1. Для каждого слова $w = 1, \\dots, n_d$:\n",
    "    1. Сгенерировать тему из дискретного распределения $t \\sim \\theta_{d}$\n",
    "    1. Сгенерировать слово из дискретного распределения $w \\sim \\phi_{t}$.\n",
    "    \n",
    "Подробнее об этом в [Википедии](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation).\n",
    "\n",
    "В контексте нашей задачи получается, что, используя данный генеративный процесс, можно создавать новые рецепты. Вы можете передать в функцию модель и число ингредиентов и сгенерировать рецепт :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recipe(model, num_ingredients):\n",
    "    theta = np.random.dirichlet(model.alpha)\n",
    "    for i in range(num_ingredients):\n",
    "        t = np.random.choice(np.arange(model.num_topics), p=theta)\n",
    "        topic = model.show_topic(t, topn=model.num_terms)\n",
    "        topic_distr = [x[1] for x in topic]\n",
    "        terms = [x[0] for x in topic]\n",
    "        w = np.random.choice(terms, p=topic_distr)\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263\n",
      "166\n",
      "460\n",
      "852\n",
      "127\n",
      "89\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "generate_recipe(lda3, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Интерпретация построенной модели\n",
    "Вы можете рассмотреть топы ингредиентов каждой темы. Большиснтво тем сами по себе похожи на рецепты; в некоторых собираются продукты одного вида, например, свежие фрукты или разные виды сыра.\n",
    "\n",
    "Попробуем эмпирически соотнести наши темы с национальными кухнями (cuisine). Построим матрицу $A$ размера темы $x$ кухни, ее элементы $a_{tc}$ — суммы $p(t|d)$ по всем документам $d$, которые отнесены к кухне $c$. Нормируем матрицу на частоты рецептов по разным кухням, чтобы избежать дисбаланса между кухнями. Следующая функция получает на вход объект модели, объект корпуса и исходные данные и возвращает нормированную матрицу $A$. Ее удобно визуализировать с помощью seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agekyan/.conda/envs/psheenv/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import seaborn\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_topic_cuisine_matrix(model, corpus, recipes):\n",
    "    # составляем вектор целевых признаков\n",
    "    targets = list(set([recipe[\"cuisine\"] for recipe in recipes]))\n",
    "    # составляем матрицу\n",
    "    tc_matrix = pandas.DataFrame(data=np.zeros((model.num_topics, len(targets))), columns=targets)\n",
    "    for recipe, bow in zip(recipes, corpus):\n",
    "        recipe_topic = model.get_document_topics(bow)\n",
    "        for t, prob in recipe_topic:\n",
    "            tc_matrix[recipe[\"cuisine\"]][t] += prob\n",
    "    # нормируем матрицу\n",
    "    target_sums = pandas.DataFrame(data=np.zeros((1, len(targets))), columns=targets)\n",
    "    for recipe in recipes:\n",
    "        target_sums[recipe[\"cuisine\"]] += 1\n",
    "    return pandas.DataFrame(tc_matrix.values/target_sums.values, columns=tc_matrix.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matrix(tc_matrix):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    seaborn.heatmap(tc_matrix, square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализируйте матрицу\n",
    "tc_matrix = compute_topic_cuisine_matrix(lda3, corpus2, recipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>moroccan</th>\n",
       "      <th>brazilian</th>\n",
       "      <th>korean</th>\n",
       "      <th>french</th>\n",
       "      <th>irish</th>\n",
       "      <th>indian</th>\n",
       "      <th>jamaican</th>\n",
       "      <th>greek</th>\n",
       "      <th>spanish</th>\n",
       "      <th>filipino</th>\n",
       "      <th>russian</th>\n",
       "      <th>southern_us</th>\n",
       "      <th>cajun_creole</th>\n",
       "      <th>japanese</th>\n",
       "      <th>italian</th>\n",
       "      <th>british</th>\n",
       "      <th>vietnamese</th>\n",
       "      <th>thai</th>\n",
       "      <th>mexican</th>\n",
       "      <th>chinese</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.024047</td>\n",
       "      <td>0.024544</td>\n",
       "      <td>0.023506</td>\n",
       "      <td>0.027036</td>\n",
       "      <td>0.024981</td>\n",
       "      <td>0.022497</td>\n",
       "      <td>0.025676</td>\n",
       "      <td>0.023109</td>\n",
       "      <td>0.027159</td>\n",
       "      <td>0.024947</td>\n",
       "      <td>0.024513</td>\n",
       "      <td>0.025104</td>\n",
       "      <td>0.028674</td>\n",
       "      <td>0.022879</td>\n",
       "      <td>0.024540</td>\n",
       "      <td>0.023671</td>\n",
       "      <td>0.022307</td>\n",
       "      <td>0.022827</td>\n",
       "      <td>0.024059</td>\n",
       "      <td>0.024019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.022681</td>\n",
       "      <td>0.025139</td>\n",
       "      <td>0.023034</td>\n",
       "      <td>0.025315</td>\n",
       "      <td>0.026050</td>\n",
       "      <td>0.021937</td>\n",
       "      <td>0.028190</td>\n",
       "      <td>0.024064</td>\n",
       "      <td>0.023920</td>\n",
       "      <td>0.024639</td>\n",
       "      <td>0.024592</td>\n",
       "      <td>0.027108</td>\n",
       "      <td>0.023541</td>\n",
       "      <td>0.023228</td>\n",
       "      <td>0.024243</td>\n",
       "      <td>0.027796</td>\n",
       "      <td>0.022701</td>\n",
       "      <td>0.022674</td>\n",
       "      <td>0.023176</td>\n",
       "      <td>0.022639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.026148</td>\n",
       "      <td>0.024028</td>\n",
       "      <td>0.022655</td>\n",
       "      <td>0.023627</td>\n",
       "      <td>0.026003</td>\n",
       "      <td>0.025295</td>\n",
       "      <td>0.033920</td>\n",
       "      <td>0.024556</td>\n",
       "      <td>0.024193</td>\n",
       "      <td>0.023962</td>\n",
       "      <td>0.024110</td>\n",
       "      <td>0.024157</td>\n",
       "      <td>0.025530</td>\n",
       "      <td>0.022594</td>\n",
       "      <td>0.022648</td>\n",
       "      <td>0.025527</td>\n",
       "      <td>0.025831</td>\n",
       "      <td>0.025701</td>\n",
       "      <td>0.024528</td>\n",
       "      <td>0.022347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.024848</td>\n",
       "      <td>0.027164</td>\n",
       "      <td>0.022500</td>\n",
       "      <td>0.021945</td>\n",
       "      <td>0.023326</td>\n",
       "      <td>0.025620</td>\n",
       "      <td>0.025228</td>\n",
       "      <td>0.023330</td>\n",
       "      <td>0.024720</td>\n",
       "      <td>0.023831</td>\n",
       "      <td>0.021925</td>\n",
       "      <td>0.023755</td>\n",
       "      <td>0.024650</td>\n",
       "      <td>0.023136</td>\n",
       "      <td>0.022385</td>\n",
       "      <td>0.022847</td>\n",
       "      <td>0.025284</td>\n",
       "      <td>0.024758</td>\n",
       "      <td>0.043795</td>\n",
       "      <td>0.021794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.025253</td>\n",
       "      <td>0.025154</td>\n",
       "      <td>0.022986</td>\n",
       "      <td>0.023358</td>\n",
       "      <td>0.022877</td>\n",
       "      <td>0.033053</td>\n",
       "      <td>0.024417</td>\n",
       "      <td>0.024061</td>\n",
       "      <td>0.025066</td>\n",
       "      <td>0.028527</td>\n",
       "      <td>0.024429</td>\n",
       "      <td>0.022912</td>\n",
       "      <td>0.022958</td>\n",
       "      <td>0.025407</td>\n",
       "      <td>0.024073</td>\n",
       "      <td>0.023159</td>\n",
       "      <td>0.025921</td>\n",
       "      <td>0.024569</td>\n",
       "      <td>0.025058</td>\n",
       "      <td>0.025247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.022156</td>\n",
       "      <td>0.024043</td>\n",
       "      <td>0.025664</td>\n",
       "      <td>0.025615</td>\n",
       "      <td>0.023154</td>\n",
       "      <td>0.023722</td>\n",
       "      <td>0.023848</td>\n",
       "      <td>0.022536</td>\n",
       "      <td>0.023131</td>\n",
       "      <td>0.030021</td>\n",
       "      <td>0.024530</td>\n",
       "      <td>0.022885</td>\n",
       "      <td>0.022998</td>\n",
       "      <td>0.024073</td>\n",
       "      <td>0.023825</td>\n",
       "      <td>0.023706</td>\n",
       "      <td>0.032881</td>\n",
       "      <td>0.035836</td>\n",
       "      <td>0.022402</td>\n",
       "      <td>0.027361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.024183</td>\n",
       "      <td>0.027422</td>\n",
       "      <td>0.023665</td>\n",
       "      <td>0.033469</td>\n",
       "      <td>0.039260</td>\n",
       "      <td>0.023029</td>\n",
       "      <td>0.025679</td>\n",
       "      <td>0.025687</td>\n",
       "      <td>0.026429</td>\n",
       "      <td>0.024172</td>\n",
       "      <td>0.033660</td>\n",
       "      <td>0.046252</td>\n",
       "      <td>0.026053</td>\n",
       "      <td>0.024374</td>\n",
       "      <td>0.026342</td>\n",
       "      <td>0.040534</td>\n",
       "      <td>0.022263</td>\n",
       "      <td>0.022109</td>\n",
       "      <td>0.023427</td>\n",
       "      <td>0.022842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.021727</td>\n",
       "      <td>0.022897</td>\n",
       "      <td>0.021627</td>\n",
       "      <td>0.024824</td>\n",
       "      <td>0.023401</td>\n",
       "      <td>0.020682</td>\n",
       "      <td>0.021247</td>\n",
       "      <td>0.026420</td>\n",
       "      <td>0.023731</td>\n",
       "      <td>0.022579</td>\n",
       "      <td>0.022231</td>\n",
       "      <td>0.023461</td>\n",
       "      <td>0.023114</td>\n",
       "      <td>0.021866</td>\n",
       "      <td>0.041059</td>\n",
       "      <td>0.022501</td>\n",
       "      <td>0.020815</td>\n",
       "      <td>0.020898</td>\n",
       "      <td>0.021846</td>\n",
       "      <td>0.020778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.021082</td>\n",
       "      <td>0.024649</td>\n",
       "      <td>0.022167</td>\n",
       "      <td>0.022428</td>\n",
       "      <td>0.023463</td>\n",
       "      <td>0.021497</td>\n",
       "      <td>0.023271</td>\n",
       "      <td>0.023362</td>\n",
       "      <td>0.022945</td>\n",
       "      <td>0.023471</td>\n",
       "      <td>0.027781</td>\n",
       "      <td>0.024136</td>\n",
       "      <td>0.022814</td>\n",
       "      <td>0.021664</td>\n",
       "      <td>0.023299</td>\n",
       "      <td>0.023291</td>\n",
       "      <td>0.021374</td>\n",
       "      <td>0.020916</td>\n",
       "      <td>0.053103</td>\n",
       "      <td>0.021604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.021005</td>\n",
       "      <td>0.022397</td>\n",
       "      <td>0.043454</td>\n",
       "      <td>0.022748</td>\n",
       "      <td>0.022740</td>\n",
       "      <td>0.022406</td>\n",
       "      <td>0.023270</td>\n",
       "      <td>0.021695</td>\n",
       "      <td>0.022220</td>\n",
       "      <td>0.031395</td>\n",
       "      <td>0.022528</td>\n",
       "      <td>0.022927</td>\n",
       "      <td>0.021923</td>\n",
       "      <td>0.033988</td>\n",
       "      <td>0.022127</td>\n",
       "      <td>0.022687</td>\n",
       "      <td>0.034045</td>\n",
       "      <td>0.033959</td>\n",
       "      <td>0.021689</td>\n",
       "      <td>0.069147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.023158</td>\n",
       "      <td>0.024514</td>\n",
       "      <td>0.026033</td>\n",
       "      <td>0.023470</td>\n",
       "      <td>0.023384</td>\n",
       "      <td>0.022131</td>\n",
       "      <td>0.023468</td>\n",
       "      <td>0.023862</td>\n",
       "      <td>0.024307</td>\n",
       "      <td>0.023775</td>\n",
       "      <td>0.024214</td>\n",
       "      <td>0.024301</td>\n",
       "      <td>0.024712</td>\n",
       "      <td>0.024137</td>\n",
       "      <td>0.022947</td>\n",
       "      <td>0.023523</td>\n",
       "      <td>0.024295</td>\n",
       "      <td>0.023636</td>\n",
       "      <td>0.024681</td>\n",
       "      <td>0.023280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.021983</td>\n",
       "      <td>0.022907</td>\n",
       "      <td>0.031465</td>\n",
       "      <td>0.022110</td>\n",
       "      <td>0.024242</td>\n",
       "      <td>0.024045</td>\n",
       "      <td>0.022534</td>\n",
       "      <td>0.026071</td>\n",
       "      <td>0.023183</td>\n",
       "      <td>0.025672</td>\n",
       "      <td>0.024740</td>\n",
       "      <td>0.022640</td>\n",
       "      <td>0.021303</td>\n",
       "      <td>0.033952</td>\n",
       "      <td>0.022305</td>\n",
       "      <td>0.022791</td>\n",
       "      <td>0.035114</td>\n",
       "      <td>0.030047</td>\n",
       "      <td>0.022583</td>\n",
       "      <td>0.029742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.024029</td>\n",
       "      <td>0.030166</td>\n",
       "      <td>0.022114</td>\n",
       "      <td>0.037406</td>\n",
       "      <td>0.029881</td>\n",
       "      <td>0.023283</td>\n",
       "      <td>0.026177</td>\n",
       "      <td>0.024620</td>\n",
       "      <td>0.026202</td>\n",
       "      <td>0.028427</td>\n",
       "      <td>0.036029</td>\n",
       "      <td>0.031748</td>\n",
       "      <td>0.025795</td>\n",
       "      <td>0.024697</td>\n",
       "      <td>0.028219</td>\n",
       "      <td>0.036402</td>\n",
       "      <td>0.022831</td>\n",
       "      <td>0.020991</td>\n",
       "      <td>0.023462</td>\n",
       "      <td>0.023569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.023054</td>\n",
       "      <td>0.023241</td>\n",
       "      <td>0.022290</td>\n",
       "      <td>0.024850</td>\n",
       "      <td>0.023563</td>\n",
       "      <td>0.021657</td>\n",
       "      <td>0.023754</td>\n",
       "      <td>0.024525</td>\n",
       "      <td>0.025387</td>\n",
       "      <td>0.023147</td>\n",
       "      <td>0.023789</td>\n",
       "      <td>0.023917</td>\n",
       "      <td>0.023702</td>\n",
       "      <td>0.023041</td>\n",
       "      <td>0.027158</td>\n",
       "      <td>0.023757</td>\n",
       "      <td>0.021947</td>\n",
       "      <td>0.022326</td>\n",
       "      <td>0.023157</td>\n",
       "      <td>0.022360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.025243</td>\n",
       "      <td>0.025245</td>\n",
       "      <td>0.023684</td>\n",
       "      <td>0.022954</td>\n",
       "      <td>0.023047</td>\n",
       "      <td>0.030111</td>\n",
       "      <td>0.027838</td>\n",
       "      <td>0.023050</td>\n",
       "      <td>0.023485</td>\n",
       "      <td>0.024971</td>\n",
       "      <td>0.022361</td>\n",
       "      <td>0.023414</td>\n",
       "      <td>0.023994</td>\n",
       "      <td>0.023276</td>\n",
       "      <td>0.022939</td>\n",
       "      <td>0.022912</td>\n",
       "      <td>0.025677</td>\n",
       "      <td>0.027402</td>\n",
       "      <td>0.024724</td>\n",
       "      <td>0.023798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.026215</td>\n",
       "      <td>0.023513</td>\n",
       "      <td>0.023900</td>\n",
       "      <td>0.028182</td>\n",
       "      <td>0.025047</td>\n",
       "      <td>0.021283</td>\n",
       "      <td>0.022690</td>\n",
       "      <td>0.028070</td>\n",
       "      <td>0.031251</td>\n",
       "      <td>0.022465</td>\n",
       "      <td>0.023476</td>\n",
       "      <td>0.023250</td>\n",
       "      <td>0.023027</td>\n",
       "      <td>0.022614</td>\n",
       "      <td>0.033793</td>\n",
       "      <td>0.024344</td>\n",
       "      <td>0.021693</td>\n",
       "      <td>0.021829</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>0.021529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.028191</td>\n",
       "      <td>0.024067</td>\n",
       "      <td>0.023265</td>\n",
       "      <td>0.023641</td>\n",
       "      <td>0.023045</td>\n",
       "      <td>0.026420</td>\n",
       "      <td>0.023093</td>\n",
       "      <td>0.026391</td>\n",
       "      <td>0.023778</td>\n",
       "      <td>0.024044</td>\n",
       "      <td>0.025260</td>\n",
       "      <td>0.023500</td>\n",
       "      <td>0.023452</td>\n",
       "      <td>0.024712</td>\n",
       "      <td>0.023408</td>\n",
       "      <td>0.023354</td>\n",
       "      <td>0.023092</td>\n",
       "      <td>0.023197</td>\n",
       "      <td>0.024223</td>\n",
       "      <td>0.022496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.024287</td>\n",
       "      <td>0.023586</td>\n",
       "      <td>0.021695</td>\n",
       "      <td>0.023986</td>\n",
       "      <td>0.024014</td>\n",
       "      <td>0.022943</td>\n",
       "      <td>0.022878</td>\n",
       "      <td>0.026575</td>\n",
       "      <td>0.026437</td>\n",
       "      <td>0.023395</td>\n",
       "      <td>0.025409</td>\n",
       "      <td>0.024372</td>\n",
       "      <td>0.027656</td>\n",
       "      <td>0.022079</td>\n",
       "      <td>0.026466</td>\n",
       "      <td>0.022768</td>\n",
       "      <td>0.021593</td>\n",
       "      <td>0.022726</td>\n",
       "      <td>0.025399</td>\n",
       "      <td>0.021961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.021830</td>\n",
       "      <td>0.028509</td>\n",
       "      <td>0.022081</td>\n",
       "      <td>0.024928</td>\n",
       "      <td>0.026247</td>\n",
       "      <td>0.024344</td>\n",
       "      <td>0.025411</td>\n",
       "      <td>0.022692</td>\n",
       "      <td>0.024696</td>\n",
       "      <td>0.026746</td>\n",
       "      <td>0.023919</td>\n",
       "      <td>0.026889</td>\n",
       "      <td>0.040706</td>\n",
       "      <td>0.022065</td>\n",
       "      <td>0.023169</td>\n",
       "      <td>0.023291</td>\n",
       "      <td>0.023096</td>\n",
       "      <td>0.021774</td>\n",
       "      <td>0.023517</td>\n",
       "      <td>0.022209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.030142</td>\n",
       "      <td>0.023646</td>\n",
       "      <td>0.021937</td>\n",
       "      <td>0.029847</td>\n",
       "      <td>0.025625</td>\n",
       "      <td>0.023104</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.035281</td>\n",
       "      <td>0.032454</td>\n",
       "      <td>0.022257</td>\n",
       "      <td>0.029642</td>\n",
       "      <td>0.024206</td>\n",
       "      <td>0.024201</td>\n",
       "      <td>0.022389</td>\n",
       "      <td>0.030198</td>\n",
       "      <td>0.023772</td>\n",
       "      <td>0.021941</td>\n",
       "      <td>0.023201</td>\n",
       "      <td>0.022006</td>\n",
       "      <td>0.021741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.022891</td>\n",
       "      <td>0.030606</td>\n",
       "      <td>0.023090</td>\n",
       "      <td>0.026135</td>\n",
       "      <td>0.028656</td>\n",
       "      <td>0.022666</td>\n",
       "      <td>0.027424</td>\n",
       "      <td>0.024890</td>\n",
       "      <td>0.024560</td>\n",
       "      <td>0.024437</td>\n",
       "      <td>0.029232</td>\n",
       "      <td>0.027943</td>\n",
       "      <td>0.023279</td>\n",
       "      <td>0.024619</td>\n",
       "      <td>0.025478</td>\n",
       "      <td>0.034938</td>\n",
       "      <td>0.022286</td>\n",
       "      <td>0.021722</td>\n",
       "      <td>0.022649</td>\n",
       "      <td>0.023129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.024618</td>\n",
       "      <td>0.023849</td>\n",
       "      <td>0.025268</td>\n",
       "      <td>0.022880</td>\n",
       "      <td>0.024559</td>\n",
       "      <td>0.026441</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.023956</td>\n",
       "      <td>0.023626</td>\n",
       "      <td>0.025617</td>\n",
       "      <td>0.023194</td>\n",
       "      <td>0.023364</td>\n",
       "      <td>0.023645</td>\n",
       "      <td>0.024921</td>\n",
       "      <td>0.023151</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.025419</td>\n",
       "      <td>0.027008</td>\n",
       "      <td>0.023701</td>\n",
       "      <td>0.027454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.022947</td>\n",
       "      <td>0.022850</td>\n",
       "      <td>0.055339</td>\n",
       "      <td>0.022790</td>\n",
       "      <td>0.023912</td>\n",
       "      <td>0.021383</td>\n",
       "      <td>0.024711</td>\n",
       "      <td>0.022498</td>\n",
       "      <td>0.021729</td>\n",
       "      <td>0.030531</td>\n",
       "      <td>0.023467</td>\n",
       "      <td>0.022870</td>\n",
       "      <td>0.021567</td>\n",
       "      <td>0.049835</td>\n",
       "      <td>0.022114</td>\n",
       "      <td>0.023096</td>\n",
       "      <td>0.026860</td>\n",
       "      <td>0.026191</td>\n",
       "      <td>0.022514</td>\n",
       "      <td>0.033421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.056934</td>\n",
       "      <td>0.024046</td>\n",
       "      <td>0.022424</td>\n",
       "      <td>0.023461</td>\n",
       "      <td>0.025250</td>\n",
       "      <td>0.030119</td>\n",
       "      <td>0.028218</td>\n",
       "      <td>0.032481</td>\n",
       "      <td>0.024279</td>\n",
       "      <td>0.022842</td>\n",
       "      <td>0.024499</td>\n",
       "      <td>0.025264</td>\n",
       "      <td>0.022486</td>\n",
       "      <td>0.022136</td>\n",
       "      <td>0.022904</td>\n",
       "      <td>0.026221</td>\n",
       "      <td>0.021302</td>\n",
       "      <td>0.022239</td>\n",
       "      <td>0.024753</td>\n",
       "      <td>0.021981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.022124</td>\n",
       "      <td>0.024532</td>\n",
       "      <td>0.023086</td>\n",
       "      <td>0.023421</td>\n",
       "      <td>0.024418</td>\n",
       "      <td>0.021922</td>\n",
       "      <td>0.024108</td>\n",
       "      <td>0.022798</td>\n",
       "      <td>0.023167</td>\n",
       "      <td>0.024486</td>\n",
       "      <td>0.026544</td>\n",
       "      <td>0.026058</td>\n",
       "      <td>0.025485</td>\n",
       "      <td>0.023592</td>\n",
       "      <td>0.023327</td>\n",
       "      <td>0.024158</td>\n",
       "      <td>0.023189</td>\n",
       "      <td>0.022042</td>\n",
       "      <td>0.027900</td>\n",
       "      <td>0.022493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.024012</td>\n",
       "      <td>0.027440</td>\n",
       "      <td>0.024999</td>\n",
       "      <td>0.023414</td>\n",
       "      <td>0.023901</td>\n",
       "      <td>0.022894</td>\n",
       "      <td>0.022948</td>\n",
       "      <td>0.023224</td>\n",
       "      <td>0.024293</td>\n",
       "      <td>0.025375</td>\n",
       "      <td>0.023373</td>\n",
       "      <td>0.023216</td>\n",
       "      <td>0.024191</td>\n",
       "      <td>0.024154</td>\n",
       "      <td>0.023086</td>\n",
       "      <td>0.023209</td>\n",
       "      <td>0.029175</td>\n",
       "      <td>0.027593</td>\n",
       "      <td>0.023146</td>\n",
       "      <td>0.025157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.022932</td>\n",
       "      <td>0.024781</td>\n",
       "      <td>0.021572</td>\n",
       "      <td>0.026594</td>\n",
       "      <td>0.025566</td>\n",
       "      <td>0.021726</td>\n",
       "      <td>0.023053</td>\n",
       "      <td>0.024125</td>\n",
       "      <td>0.024478</td>\n",
       "      <td>0.022626</td>\n",
       "      <td>0.025179</td>\n",
       "      <td>0.025560</td>\n",
       "      <td>0.027466</td>\n",
       "      <td>0.022713</td>\n",
       "      <td>0.025173</td>\n",
       "      <td>0.026221</td>\n",
       "      <td>0.021596</td>\n",
       "      <td>0.021144</td>\n",
       "      <td>0.022557</td>\n",
       "      <td>0.021655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.023691</td>\n",
       "      <td>0.027380</td>\n",
       "      <td>0.021668</td>\n",
       "      <td>0.031394</td>\n",
       "      <td>0.028462</td>\n",
       "      <td>0.021913</td>\n",
       "      <td>0.023636</td>\n",
       "      <td>0.024703</td>\n",
       "      <td>0.027767</td>\n",
       "      <td>0.022917</td>\n",
       "      <td>0.026179</td>\n",
       "      <td>0.024731</td>\n",
       "      <td>0.022703</td>\n",
       "      <td>0.022354</td>\n",
       "      <td>0.025622</td>\n",
       "      <td>0.029586</td>\n",
       "      <td>0.021301</td>\n",
       "      <td>0.021245</td>\n",
       "      <td>0.022191</td>\n",
       "      <td>0.021477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.025402</td>\n",
       "      <td>0.025406</td>\n",
       "      <td>0.023290</td>\n",
       "      <td>0.023142</td>\n",
       "      <td>0.023675</td>\n",
       "      <td>0.024902</td>\n",
       "      <td>0.025820</td>\n",
       "      <td>0.023505</td>\n",
       "      <td>0.025422</td>\n",
       "      <td>0.023773</td>\n",
       "      <td>0.023105</td>\n",
       "      <td>0.025873</td>\n",
       "      <td>0.029619</td>\n",
       "      <td>0.023311</td>\n",
       "      <td>0.023113</td>\n",
       "      <td>0.023732</td>\n",
       "      <td>0.022901</td>\n",
       "      <td>0.023107</td>\n",
       "      <td>0.023807</td>\n",
       "      <td>0.023257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.022955</td>\n",
       "      <td>0.023735</td>\n",
       "      <td>0.022493</td>\n",
       "      <td>0.025722</td>\n",
       "      <td>0.024894</td>\n",
       "      <td>0.022533</td>\n",
       "      <td>0.022012</td>\n",
       "      <td>0.024674</td>\n",
       "      <td>0.027479</td>\n",
       "      <td>0.024357</td>\n",
       "      <td>0.024090</td>\n",
       "      <td>0.023202</td>\n",
       "      <td>0.022991</td>\n",
       "      <td>0.023671</td>\n",
       "      <td>0.026533</td>\n",
       "      <td>0.023994</td>\n",
       "      <td>0.023371</td>\n",
       "      <td>0.023336</td>\n",
       "      <td>0.022617</td>\n",
       "      <td>0.022869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.027144</td>\n",
       "      <td>0.023977</td>\n",
       "      <td>0.021579</td>\n",
       "      <td>0.024612</td>\n",
       "      <td>0.023304</td>\n",
       "      <td>0.022978</td>\n",
       "      <td>0.022835</td>\n",
       "      <td>0.025177</td>\n",
       "      <td>0.026679</td>\n",
       "      <td>0.022703</td>\n",
       "      <td>0.023523</td>\n",
       "      <td>0.023753</td>\n",
       "      <td>0.024299</td>\n",
       "      <td>0.021953</td>\n",
       "      <td>0.024844</td>\n",
       "      <td>0.022972</td>\n",
       "      <td>0.023205</td>\n",
       "      <td>0.023093</td>\n",
       "      <td>0.028698</td>\n",
       "      <td>0.021684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.024084</td>\n",
       "      <td>0.023477</td>\n",
       "      <td>0.022809</td>\n",
       "      <td>0.024267</td>\n",
       "      <td>0.022417</td>\n",
       "      <td>0.022604</td>\n",
       "      <td>0.024982</td>\n",
       "      <td>0.033047</td>\n",
       "      <td>0.025972</td>\n",
       "      <td>0.023035</td>\n",
       "      <td>0.023116</td>\n",
       "      <td>0.023338</td>\n",
       "      <td>0.024037</td>\n",
       "      <td>0.022064</td>\n",
       "      <td>0.026105</td>\n",
       "      <td>0.021998</td>\n",
       "      <td>0.022254</td>\n",
       "      <td>0.022366</td>\n",
       "      <td>0.026482</td>\n",
       "      <td>0.021653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.024226</td>\n",
       "      <td>0.023884</td>\n",
       "      <td>0.025976</td>\n",
       "      <td>0.025593</td>\n",
       "      <td>0.029290</td>\n",
       "      <td>0.024910</td>\n",
       "      <td>0.028111</td>\n",
       "      <td>0.023644</td>\n",
       "      <td>0.026020</td>\n",
       "      <td>0.025813</td>\n",
       "      <td>0.028132</td>\n",
       "      <td>0.023897</td>\n",
       "      <td>0.024079</td>\n",
       "      <td>0.025546</td>\n",
       "      <td>0.024116</td>\n",
       "      <td>0.026096</td>\n",
       "      <td>0.024246</td>\n",
       "      <td>0.023112</td>\n",
       "      <td>0.023104</td>\n",
       "      <td>0.023931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.022091</td>\n",
       "      <td>0.022816</td>\n",
       "      <td>0.024018</td>\n",
       "      <td>0.024376</td>\n",
       "      <td>0.024973</td>\n",
       "      <td>0.021927</td>\n",
       "      <td>0.023890</td>\n",
       "      <td>0.024536</td>\n",
       "      <td>0.023289</td>\n",
       "      <td>0.025624</td>\n",
       "      <td>0.023604</td>\n",
       "      <td>0.025464</td>\n",
       "      <td>0.023284</td>\n",
       "      <td>0.025040</td>\n",
       "      <td>0.025375</td>\n",
       "      <td>0.023565</td>\n",
       "      <td>0.023545</td>\n",
       "      <td>0.022150</td>\n",
       "      <td>0.023596</td>\n",
       "      <td>0.024817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.021699</td>\n",
       "      <td>0.031359</td>\n",
       "      <td>0.024634</td>\n",
       "      <td>0.021830</td>\n",
       "      <td>0.022081</td>\n",
       "      <td>0.025048</td>\n",
       "      <td>0.029495</td>\n",
       "      <td>0.021536</td>\n",
       "      <td>0.022019</td>\n",
       "      <td>0.030124</td>\n",
       "      <td>0.021841</td>\n",
       "      <td>0.022526</td>\n",
       "      <td>0.021194</td>\n",
       "      <td>0.023874</td>\n",
       "      <td>0.021596</td>\n",
       "      <td>0.021912</td>\n",
       "      <td>0.051264</td>\n",
       "      <td>0.062051</td>\n",
       "      <td>0.025115</td>\n",
       "      <td>0.024814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.021677</td>\n",
       "      <td>0.023276</td>\n",
       "      <td>0.036150</td>\n",
       "      <td>0.022491</td>\n",
       "      <td>0.022871</td>\n",
       "      <td>0.023600</td>\n",
       "      <td>0.026804</td>\n",
       "      <td>0.022215</td>\n",
       "      <td>0.022343</td>\n",
       "      <td>0.024886</td>\n",
       "      <td>0.023075</td>\n",
       "      <td>0.022718</td>\n",
       "      <td>0.022910</td>\n",
       "      <td>0.035761</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>0.023014</td>\n",
       "      <td>0.028826</td>\n",
       "      <td>0.028507</td>\n",
       "      <td>0.022920</td>\n",
       "      <td>0.037701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.036789</td>\n",
       "      <td>0.023532</td>\n",
       "      <td>0.022766</td>\n",
       "      <td>0.023785</td>\n",
       "      <td>0.024202</td>\n",
       "      <td>0.075136</td>\n",
       "      <td>0.027214</td>\n",
       "      <td>0.023978</td>\n",
       "      <td>0.023878</td>\n",
       "      <td>0.025558</td>\n",
       "      <td>0.023625</td>\n",
       "      <td>0.023013</td>\n",
       "      <td>0.021571</td>\n",
       "      <td>0.028675</td>\n",
       "      <td>0.022275</td>\n",
       "      <td>0.025036</td>\n",
       "      <td>0.027887</td>\n",
       "      <td>0.025496</td>\n",
       "      <td>0.023387</td>\n",
       "      <td>0.023906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.023567</td>\n",
       "      <td>0.025731</td>\n",
       "      <td>0.021982</td>\n",
       "      <td>0.024870</td>\n",
       "      <td>0.026400</td>\n",
       "      <td>0.022534</td>\n",
       "      <td>0.027065</td>\n",
       "      <td>0.028503</td>\n",
       "      <td>0.026277</td>\n",
       "      <td>0.024930</td>\n",
       "      <td>0.026611</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.041159</td>\n",
       "      <td>0.022436</td>\n",
       "      <td>0.028684</td>\n",
       "      <td>0.025261</td>\n",
       "      <td>0.021543</td>\n",
       "      <td>0.021070</td>\n",
       "      <td>0.025209</td>\n",
       "      <td>0.022351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.024969</td>\n",
       "      <td>0.025276</td>\n",
       "      <td>0.024398</td>\n",
       "      <td>0.024496</td>\n",
       "      <td>0.024098</td>\n",
       "      <td>0.023585</td>\n",
       "      <td>0.024579</td>\n",
       "      <td>0.027103</td>\n",
       "      <td>0.026617</td>\n",
       "      <td>0.024083</td>\n",
       "      <td>0.024541</td>\n",
       "      <td>0.023857</td>\n",
       "      <td>0.024815</td>\n",
       "      <td>0.023394</td>\n",
       "      <td>0.025426</td>\n",
       "      <td>0.024579</td>\n",
       "      <td>0.022422</td>\n",
       "      <td>0.022677</td>\n",
       "      <td>0.023011</td>\n",
       "      <td>0.022727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.023985</td>\n",
       "      <td>0.025214</td>\n",
       "      <td>0.022738</td>\n",
       "      <td>0.022984</td>\n",
       "      <td>0.023719</td>\n",
       "      <td>0.022120</td>\n",
       "      <td>0.023767</td>\n",
       "      <td>0.023447</td>\n",
       "      <td>0.025412</td>\n",
       "      <td>0.023840</td>\n",
       "      <td>0.023930</td>\n",
       "      <td>0.027418</td>\n",
       "      <td>0.028415</td>\n",
       "      <td>0.023778</td>\n",
       "      <td>0.023531</td>\n",
       "      <td>0.024525</td>\n",
       "      <td>0.022706</td>\n",
       "      <td>0.022476</td>\n",
       "      <td>0.023807</td>\n",
       "      <td>0.023058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    moroccan  brazilian    korean    french     irish    indian  jamaican  \\\n",
       "0   0.024047   0.024544  0.023506  0.027036  0.024981  0.022497  0.025676   \n",
       "1   0.022681   0.025139  0.023034  0.025315  0.026050  0.021937  0.028190   \n",
       "2   0.026148   0.024028  0.022655  0.023627  0.026003  0.025295  0.033920   \n",
       "3   0.024848   0.027164  0.022500  0.021945  0.023326  0.025620  0.025228   \n",
       "4   0.025253   0.025154  0.022986  0.023358  0.022877  0.033053  0.024417   \n",
       "5   0.022156   0.024043  0.025664  0.025615  0.023154  0.023722  0.023848   \n",
       "6   0.024183   0.027422  0.023665  0.033469  0.039260  0.023029  0.025679   \n",
       "7   0.021727   0.022897  0.021627  0.024824  0.023401  0.020682  0.021247   \n",
       "8   0.021082   0.024649  0.022167  0.022428  0.023463  0.021497  0.023271   \n",
       "9   0.021005   0.022397  0.043454  0.022748  0.022740  0.022406  0.023270   \n",
       "10  0.023158   0.024514  0.026033  0.023470  0.023384  0.022131  0.023468   \n",
       "11  0.021983   0.022907  0.031465  0.022110  0.024242  0.024045  0.022534   \n",
       "12  0.024029   0.030166  0.022114  0.037406  0.029881  0.023283  0.026177   \n",
       "13  0.023054   0.023241  0.022290  0.024850  0.023563  0.021657  0.023754   \n",
       "14  0.025243   0.025245  0.023684  0.022954  0.023047  0.030111  0.027838   \n",
       "15  0.026215   0.023513  0.023900  0.028182  0.025047  0.021283  0.022690   \n",
       "16  0.028191   0.024067  0.023265  0.023641  0.023045  0.026420  0.023093   \n",
       "17  0.024287   0.023586  0.021695  0.023986  0.024014  0.022943  0.022878   \n",
       "18  0.021830   0.028509  0.022081  0.024928  0.026247  0.024344  0.025411   \n",
       "19  0.030142   0.023646  0.021937  0.029847  0.025625  0.023104  0.021739   \n",
       "20  0.022891   0.030606  0.023090  0.026135  0.028656  0.022666  0.027424   \n",
       "21  0.024618   0.023849  0.025268  0.022880  0.024559  0.026441  0.025000   \n",
       "22  0.022947   0.022850  0.055339  0.022790  0.023912  0.021383  0.024711   \n",
       "23  0.056934   0.024046  0.022424  0.023461  0.025250  0.030119  0.028218   \n",
       "24  0.022124   0.024532  0.023086  0.023421  0.024418  0.021922  0.024108   \n",
       "25  0.024012   0.027440  0.024999  0.023414  0.023901  0.022894  0.022948   \n",
       "26  0.022932   0.024781  0.021572  0.026594  0.025566  0.021726  0.023053   \n",
       "27  0.023691   0.027380  0.021668  0.031394  0.028462  0.021913  0.023636   \n",
       "28  0.025402   0.025406  0.023290  0.023142  0.023675  0.024902  0.025820   \n",
       "29  0.022955   0.023735  0.022493  0.025722  0.024894  0.022533  0.022012   \n",
       "30  0.027144   0.023977  0.021579  0.024612  0.023304  0.022978  0.022835   \n",
       "31  0.024084   0.023477  0.022809  0.024267  0.022417  0.022604  0.024982   \n",
       "32  0.024226   0.023884  0.025976  0.025593  0.029290  0.024910  0.028111   \n",
       "33  0.022091   0.022816  0.024018  0.024376  0.024973  0.021927  0.023890   \n",
       "34  0.021699   0.031359  0.024634  0.021830  0.022081  0.025048  0.029495   \n",
       "35  0.021677   0.023276  0.036150  0.022491  0.022871  0.023600  0.026804   \n",
       "36  0.036789   0.023532  0.022766  0.023785  0.024202  0.075136  0.027214   \n",
       "37  0.023567   0.025731  0.021982  0.024870  0.026400  0.022534  0.027065   \n",
       "38  0.024969   0.025276  0.024398  0.024496  0.024098  0.023585  0.024579   \n",
       "39  0.023985   0.025214  0.022738  0.022984  0.023719  0.022120  0.023767   \n",
       "\n",
       "       greek   spanish  filipino   russian  southern_us  cajun_creole  \\\n",
       "0   0.023109  0.027159  0.024947  0.024513     0.025104      0.028674   \n",
       "1   0.024064  0.023920  0.024639  0.024592     0.027108      0.023541   \n",
       "2   0.024556  0.024193  0.023962  0.024110     0.024157      0.025530   \n",
       "3   0.023330  0.024720  0.023831  0.021925     0.023755      0.024650   \n",
       "4   0.024061  0.025066  0.028527  0.024429     0.022912      0.022958   \n",
       "5   0.022536  0.023131  0.030021  0.024530     0.022885      0.022998   \n",
       "6   0.025687  0.026429  0.024172  0.033660     0.046252      0.026053   \n",
       "7   0.026420  0.023731  0.022579  0.022231     0.023461      0.023114   \n",
       "8   0.023362  0.022945  0.023471  0.027781     0.024136      0.022814   \n",
       "9   0.021695  0.022220  0.031395  0.022528     0.022927      0.021923   \n",
       "10  0.023862  0.024307  0.023775  0.024214     0.024301      0.024712   \n",
       "11  0.026071  0.023183  0.025672  0.024740     0.022640      0.021303   \n",
       "12  0.024620  0.026202  0.028427  0.036029     0.031748      0.025795   \n",
       "13  0.024525  0.025387  0.023147  0.023789     0.023917      0.023702   \n",
       "14  0.023050  0.023485  0.024971  0.022361     0.023414      0.023994   \n",
       "15  0.028070  0.031251  0.022465  0.023476     0.023250      0.023027   \n",
       "16  0.026391  0.023778  0.024044  0.025260     0.023500      0.023452   \n",
       "17  0.026575  0.026437  0.023395  0.025409     0.024372      0.027656   \n",
       "18  0.022692  0.024696  0.026746  0.023919     0.026889      0.040706   \n",
       "19  0.035281  0.032454  0.022257  0.029642     0.024206      0.024201   \n",
       "20  0.024890  0.024560  0.024437  0.029232     0.027943      0.023279   \n",
       "21  0.023956  0.023626  0.025617  0.023194     0.023364      0.023645   \n",
       "22  0.022498  0.021729  0.030531  0.023467     0.022870      0.021567   \n",
       "23  0.032481  0.024279  0.022842  0.024499     0.025264      0.022486   \n",
       "24  0.022798  0.023167  0.024486  0.026544     0.026058      0.025485   \n",
       "25  0.023224  0.024293  0.025375  0.023373     0.023216      0.024191   \n",
       "26  0.024125  0.024478  0.022626  0.025179     0.025560      0.027466   \n",
       "27  0.024703  0.027767  0.022917  0.026179     0.024731      0.022703   \n",
       "28  0.023505  0.025422  0.023773  0.023105     0.025873      0.029619   \n",
       "29  0.024674  0.027479  0.024357  0.024090     0.023202      0.022991   \n",
       "30  0.025177  0.026679  0.022703  0.023523     0.023753      0.024299   \n",
       "31  0.033047  0.025972  0.023035  0.023116     0.023338      0.024037   \n",
       "32  0.023644  0.026020  0.025813  0.028132     0.023897      0.024079   \n",
       "33  0.024536  0.023289  0.025624  0.023604     0.025464      0.023284   \n",
       "34  0.021536  0.022019  0.030124  0.021841     0.022526      0.021194   \n",
       "35  0.022215  0.022343  0.024886  0.023075     0.022718      0.022910   \n",
       "36  0.023978  0.023878  0.025558  0.023625     0.023013      0.021571   \n",
       "37  0.028503  0.026277  0.024930  0.026611     0.025000      0.041159   \n",
       "38  0.027103  0.026617  0.024083  0.024541     0.023857      0.024815   \n",
       "39  0.023447  0.025412  0.023840  0.023930     0.027418      0.028415   \n",
       "\n",
       "    japanese   italian   british  vietnamese      thai   mexican   chinese  \n",
       "0   0.022879  0.024540  0.023671    0.022307  0.022827  0.024059  0.024019  \n",
       "1   0.023228  0.024243  0.027796    0.022701  0.022674  0.023176  0.022639  \n",
       "2   0.022594  0.022648  0.025527    0.025831  0.025701  0.024528  0.022347  \n",
       "3   0.023136  0.022385  0.022847    0.025284  0.024758  0.043795  0.021794  \n",
       "4   0.025407  0.024073  0.023159    0.025921  0.024569  0.025058  0.025247  \n",
       "5   0.024073  0.023825  0.023706    0.032881  0.035836  0.022402  0.027361  \n",
       "6   0.024374  0.026342  0.040534    0.022263  0.022109  0.023427  0.022842  \n",
       "7   0.021866  0.041059  0.022501    0.020815  0.020898  0.021846  0.020778  \n",
       "8   0.021664  0.023299  0.023291    0.021374  0.020916  0.053103  0.021604  \n",
       "9   0.033988  0.022127  0.022687    0.034045  0.033959  0.021689  0.069147  \n",
       "10  0.024137  0.022947  0.023523    0.024295  0.023636  0.024681  0.023280  \n",
       "11  0.033952  0.022305  0.022791    0.035114  0.030047  0.022583  0.029742  \n",
       "12  0.024697  0.028219  0.036402    0.022831  0.020991  0.023462  0.023569  \n",
       "13  0.023041  0.027158  0.023757    0.021947  0.022326  0.023157  0.022360  \n",
       "14  0.023276  0.022939  0.022912    0.025677  0.027402  0.024724  0.023798  \n",
       "15  0.022614  0.033793  0.024344    0.021693  0.021829  0.022000  0.021529  \n",
       "16  0.024712  0.023408  0.023354    0.023092  0.023197  0.024223  0.022496  \n",
       "17  0.022079  0.026466  0.022768    0.021593  0.022726  0.025399  0.021961  \n",
       "18  0.022065  0.023169  0.023291    0.023096  0.021774  0.023517  0.022209  \n",
       "19  0.022389  0.030198  0.023772    0.021941  0.023201  0.022006  0.021741  \n",
       "20  0.024619  0.025478  0.034938    0.022286  0.021722  0.022649  0.023129  \n",
       "21  0.024921  0.023151  0.023256    0.025419  0.027008  0.023701  0.027454  \n",
       "22  0.049835  0.022114  0.023096    0.026860  0.026191  0.022514  0.033421  \n",
       "23  0.022136  0.022904  0.026221    0.021302  0.022239  0.024753  0.021981  \n",
       "24  0.023592  0.023327  0.024158    0.023189  0.022042  0.027900  0.022493  \n",
       "25  0.024154  0.023086  0.023209    0.029175  0.027593  0.023146  0.025157  \n",
       "26  0.022713  0.025173  0.026221    0.021596  0.021144  0.022557  0.021655  \n",
       "27  0.022354  0.025622  0.029586    0.021301  0.021245  0.022191  0.021477  \n",
       "28  0.023311  0.023113  0.023732    0.022901  0.023107  0.023807  0.023257  \n",
       "29  0.023671  0.026533  0.023994    0.023371  0.023336  0.022617  0.022869  \n",
       "30  0.021953  0.024844  0.022972    0.023205  0.023093  0.028698  0.021684  \n",
       "31  0.022064  0.026105  0.021998    0.022254  0.022366  0.026482  0.021653  \n",
       "32  0.025546  0.024116  0.026096    0.024246  0.023112  0.023104  0.023931  \n",
       "33  0.025040  0.025375  0.023565    0.023545  0.022150  0.023596  0.024817  \n",
       "34  0.023874  0.021596  0.021912    0.051264  0.062051  0.025115  0.024814  \n",
       "35  0.035761  0.022400  0.023014    0.028826  0.028507  0.022920  0.037701  \n",
       "36  0.028675  0.022275  0.025036    0.027887  0.025496  0.023387  0.023906  \n",
       "37  0.022436  0.028684  0.025261    0.021543  0.021070  0.025209  0.022351  \n",
       "38  0.023394  0.025426  0.024579    0.022422  0.022677  0.023011  0.022727  \n",
       "39  0.023778  0.023531  0.024525    0.022706  0.022476  0.023807  0.023058  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_matrix(tc_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чем темнее квадрат в матрице, тем больше связь этой темы с данной кухней. Мы видим, что у нас есть темы, которые связаны с несколькими кухнями. Такие темы показывают набор ингредиентов, которые популярны в кухнях нескольких народов, то есть указывают на схожесть кухонь этих народов. Некоторые темы распределены по всем кухням равномерно, они показывают наборы продуктов, которые часто используются в кулинарии всех стран. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Жаль, что в датасете нет названий рецептов, иначе темы было бы проще интерпретировать..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заключение\n",
    "В этом задании вы построили несколько моделей LDA, посмотрели, на что влияют гиперпараметры модели и как можно использовать построенную модель. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
